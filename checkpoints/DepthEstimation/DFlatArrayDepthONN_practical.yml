name: MetaDepth
model_type: DFlatArrayDepthONN_model
is_train: true
report_to: comet_ml
push_to_hub: false
num_gpu: 8
seed: 10
mixed_precision: !!str no
allow_tf32: false
tracker_project_name: MetaDepth
experiment_key: MSArrayDepthONN_practical

dflat:
  model_name: Nanocylinders_TiO2_U300H600
  array_size: [16, 24]  # cols, rows  
  array_spacing: [!!float 800e-6, !!float 800e-6] # col, row spacing (12mm x 20 mm MS array)
  downscale_factor: [1, 1]  # 100MP -> 50MP 
  h: 512
  w: 512
  in_dx_m: [!!float 1500e-9, !!float 1500e-9]
  out_dx_m: [!!float 2740e-9, !!float 2740e-9]  # pixel pitch
  pixel_dx_m: [!!float 1e-2, !!float 1e-2] # 1 pixel is 1cm (w) x 1cm (h) in real life. Is it a good assumption? Disregard this. We will assume objects as image. Thus size varies

  wavelength_set_m: [!!float 400e-9, !!float 550e-9, !!float 700e-9]
  
  out_distance_m: !!float 1e-2
  initialization:
    type: focusing_lens
    depth_set_m: [!!float 500, !!float 500, !!float 500]
    fshift_set_m: [[!!float 0.0, !!float 0.0],[!!float 0.0, !!float 0.0],[!!float 0.0, !!float 0.0]]
  
  ps_locs:
    # loc0: [-400e-6,-400e-6] # no Z value because we have variable depth
    # loc1: [-400e-6, 0.0]
    # loc2: [-400e-6, 400e-6]
    # loc3: [0.0,-400e-6]
    - [0.0, 0.0]
    # loc5: [0.0, 400e-6]
    # loc6: [400e-6,-400e-6]
    # loc7: [400e-6, 0.0]
    # loc8: [400e-6, 400e-6]
  
  
datasets:
  train:
    type: HuggingFaceDataset
    name: /depot/chan129/users/harshana/Datasets/SUNRGBD/kv2/kinect2data/
    split: train
    trust_remote_code: true

    use_shuffle: true
    num_worker_per_gpu: 4
    gt_key: gt
    lq_key: blur

    common_transforms_keys: [image, depth]
    transforms:
      to_tensor:
      select_channels:
       channels: [False, True, False]
      crop:
        h: 256
        w: 256

  val:
    type: HuggingFaceDataset
    name: /depot/chan129/users/harshana/Datasets/SUNRGBD/kv2/kinect2data/
    split: validation
    trust_remote_code: true
    
    use_shuffle: false
    gt_key: gt
    lq_key: blur
    
    common_transforms_keys: [image, depth]
    transforms:
      to_tensor:
      select_channels:
       channels: [False, True, False]
      crop:
        h: 256
        w: 256


# no training network structure
network: ~

# depth estimating network
depth_network: 
  type: file
  path: /depot/chan129/users/harshana/svd_v2/checkpoints/DepthEstimation/kernels.pt
  rescale: 1



# path
path:
  root: /scratch/gilbreth/wweligam/experiments
  logging_dir: logs
  resume_from_path: /scratch/gilbreth/wweligam/experiments/MetaDepth/MSArrayDepthONN_practical 2025 09 09 17.57.03/
  resume_from_checkpoint: latest

# training settings
train:
  optimize_shape_param: true  # optimize MS shape parameters?

  optim:
    scale_lr: false
    use_8bit_adam: false
    learning_rate: !!float 2e-4
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_weight_decay: 0.01
    adam_epsilon: !!float 1e-8
    max_grad_norm: 1.0

  scheduler:
    type: constant
    lr_warmup_steps: 5
    lr_num_cycles: 2
    lr_power: 1.0

  loss: 1*MSE
  gradient_accumulation_steps: 1
  num_train_epochs: 10
  batch_size: 1

  max_train_steps: 500
  validation_steps: 50
  checkpointing_steps: 50
  checkpoints_total_limit: 2

  patched: false
  patch_size_h: 256
  patch_size_w: 256
  patch_overlap: 0.2
  max_minibatch: 32

# validation settings
val:
  save_images: false
  patched: false
  patch_size_h: 256
  patch_size_w: 256
  patch_overlap: 0.2
  max_minibatch: 8
  batch_size: 1

  metrics:
    psnr: # metric name, can be arbitrary
      crop: 30
    mse:
       crop: 30
